<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google Expands AI Search and Announces Agent-to-Agent Protocol</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; max-width: 800px; margin: 0 auto; }
        h1 { color: #333; }
        .article-meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
        .article-content { margin-bottom: 30px; }
        .article-source { font-style: italic; }
        img { max-width: 100%; height: auto; }
    </style>
</head>
<body>
    <h1>Google Expands AI Search and Announces Agent-to-Agent Protocol</h1>
    <div class="article-meta">
        <p>Published: Thu, 17 Apr 2025</p>
    </div>
    <div class="article-content">
        <p>Google expanded its AI mode in search with improved capabilities for comparisons and how-to queries, along with the ability to handle longer, more open-ended questions. The AI search can now also see and search with images. At Google Cloud Next 25, the company announced a new TPU (tensor processing unit) coming later this year and introduced A2A, an agent-to-agent protocol that facilitates communication between AI agents to work autonomously. Google also rolled out new AI features in Workspace products, including audio features in Google Docs, a 'help me refine' feature, AI enhancements in Sheets, and Gemini features in Google Meet for meeting summaries and Q&A. Additionally, Google updated Vertex AI with new editing and camera control features in V2, and made Chirp 3 (audio generation), Imagine 3 (text-to-image), and LIIA (text-to-music) models available in Vertex AI.</p><p>Source: https://www.youtube.com/watch?v=usjPCQAoF44</p>
    </div>
    <div class="article-source">
        <p>Source: <a href="https://www.youtube.com/watch?v=usjPCQAoF44" target="_blank">https://www.youtube.com/watch?v=usjPCQAoF44</a></p>
    </div>
</body>
</html>